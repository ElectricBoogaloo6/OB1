{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabula as tb\n",
    "import pandas as pd\n",
    "import re\n",
    "import PyPDF2\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r'C:\\Users\\Konstantin\\Documents\\OB1-1\\research_papers\\Beyersmann-ea20 Morphological processing across modalities and languages TO SIMULATE-sep21.pdf'\n",
    "\n",
    "pdfFileObj = open(file, 'rb')\n",
    "pdfReader = PyPDF2.PdfReader(pdfFileObj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Last page \"words\"\n",
    "\n",
    "pageObj_19 = pdfReader.getPage(19)\n",
    "last_page = pageObj_19.extractText()\n",
    "last_page_cols = last_page.replace('Words','')\n",
    "last_page_cols = last_page_cols.replace(\"SCIENTIFIC STUDIES OF READING 519\", '')\n",
    "last_page_cols_no_spaces = last_page_cols.replace(' ', ',')\n",
    "df_last_page = pd.read_csv(io.StringIO(last_page_cols_no_spaces), sep=\",\")\n",
    "\n",
    "# turn to list and save text\n",
    "morph_simple_french = df_last_page['Morphologically'].tolist()\n",
    "morph_simple_french.pop(0)\n",
    "#print(morph_simple_french)\n",
    "with open('WORDS_morph_simple_french.txt', 'w') as mf:\n",
    "    for item in morph_simple_french:\n",
    "        mf.write(\"%s\\n\" % item)\n",
    "\n",
    "morph_simple_german = df_last_page['simple'].tolist()\n",
    "morph_simple_german.pop(0)\n",
    "#print(morph_simple_german)\n",
    "with open('WORDS_morph_simple_german.txt', 'w') as mg:\n",
    "    for item in morph_simple_german:\n",
    "        mg.write(\"%s\\n\" % item)\n",
    "\n",
    "morph_complex_french = df_last_page['Morphologically.1'].tolist()\n",
    "morph_complex_french.pop(0)\n",
    "#print(morph_complex_french)\n",
    "with open('WORDS_morph_complex_french.txt', 'w') as mf:\n",
    "    for item in morph_complex_french:\n",
    "        mf.write(\"%s\\n\" % item)\n",
    "\n",
    "morph_complex_german = df_last_page['complex'].tolist()\n",
    "morph_complex_german.pop(0)\n",
    "#print(morph_complex_german)\n",
    "with open('WORDS_morph_complex_german.txt', 'w') as mg:\n",
    "    for item in morph_complex_german:\n",
    "        mg.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## second last page \"non-words\"\n",
    "\n",
    "pageObj_18 = pdfReader.getPage(18)\n",
    "second_last_page = pageObj_18.extractText()\n",
    "second_last_page_cols = second_last_page.replace('Non-Stem+Suffix Non-Stem+Non-Suffix','')\n",
    "second_last_page_cols = second_last_page_cols.replace(\"518 E. BEYERSMANN ET AL.\", '')\n",
    "second_last_page_cols_no_spaces = second_last_page_cols.replace(' ', ',')\n",
    "df_second_last_page = pd.read_csv(io.StringIO(second_last_page_cols_no_spaces), sep=\",\")\n",
    "\n",
    "# turn to list and save text\n",
    "non_stem_plus_suffix_french = df_second_last_page[\"French\"].tolist()\n",
    "#non_stem_plus_suffix_french.pop(0)\n",
    "with open('NONWORDS_non_stem_plus_suffix_french.txt', 'w') as mf:\n",
    "    for item in non_stem_plus_suffix_french:\n",
    "        mf.write(\"%s\\n\" % item)\n",
    "        \n",
    "non_stem_plus_suffix_german = df_second_last_page[\"German\"].tolist()\n",
    "#non_stem_plus_suffix_german.pop(0)\n",
    "with open('NONWORDS_non_stem_plus_suffix_german.txt', 'w') as mf:\n",
    "    for item in non_stem_plus_suffix_german:\n",
    "        mf.write(\"%s\\n\" % item)\n",
    "        \n",
    "non_stem_plus_non_suffix_french = df_second_last_page[\"French.1\"].tolist()\n",
    "#non_stem_plus_non_suffix_french.pop(0)\n",
    "with open('NONWORDS_non_stem_plus_non_suffix_french.txt', 'w') as mf:\n",
    "    for item in non_stem_plus_non_suffix_french:\n",
    "        mf.write(\"%s\\n\" % item)\n",
    "        \n",
    "non_stem_plus_non_suffix_german = df_second_last_page[\"German.1\"].tolist()\n",
    "#non_stem_plus_non_suffix_german.pop(0)\n",
    "with open('NONWORDS_non_stem_plus_non_suffix_german.txt', 'w') as mf:\n",
    "    for item in non_stem_plus_non_suffix_german:\n",
    "        mf.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## third last page\n",
    "\n",
    "pageObj_17 = pdfReader.getPage(17)\n",
    "third_last_page = pageObj_17.extractText()\n",
    "third_last_page_cols = third_last_page.replace('Stem+Suffix Stem+Non-Suffix','')\n",
    "third_last_page_cols = third_last_page_cols.replace('SCIENTIFIC STUDIES OF READING 517', '')\n",
    "third_last_page_cols = third_last_page_cols.replace('Appendix Items used in the study', '')\n",
    "third_last_page_cols = third_last_page_cols.replace('Nonwords', '')\n",
    "third_last_page_cols_no_spaces = third_last_page_cols.replace(' ', ',')\n",
    "df_third_last_page = pd.read_csv(io.StringIO(third_last_page_cols_no_spaces), sep=\",\")\n",
    "\n",
    "# turn to list and save text\n",
    "stem_plus_suffix_french = df_third_last_page[\"French\"].tolist()\n",
    "#non_stem_plus_suffix_french.pop(0)\n",
    "with open('NONWORDS_stem_plus_suffix_french.txt', 'w') as mf:\n",
    "    for item in stem_plus_suffix_french:\n",
    "        mf.write(\"%s\\n\" % item)\n",
    "\n",
    "stem_plus_suffix_german = df_third_last_page[\"German\"].tolist()\n",
    "#non_stem_plus_suffix_german.pop(0)\n",
    "with open('NONWORDS_stem_plus_suffix_german.txt', 'w') as mf:\n",
    "    for item in stem_plus_suffix_german:\n",
    "        mf.write(\"%s\\n\" % item)\n",
    "        \n",
    "stem_plus_non_suffix_french = df_third_last_page[\"French.1\"].tolist()\n",
    "#non_stem_plus_suffix_french.pop(0)\n",
    "with open('NONWORDS_stem_plus_non_suffix_french.txt', 'w') as mf:\n",
    "    for item in stem_plus_non_suffix_french:\n",
    "        mf.write(\"%s\\n\" % item)\n",
    "        \n",
    "stem_plus_non_suffix_german = df_third_last_page[\"German.1\"].tolist()\n",
    "#non_stem_plus_suffix_german.pop(0)\n",
    "with open('NONWORDS_stem_plus_non_suffix_german.txt', 'w') as mf:\n",
    "    for item in stem_plus_non_suffix_german:\n",
    "        mf.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_third_last_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57c3ec6ae53a29a716a15d6ee936d222e2ef0feb9f2e15fe97c135dc96e65047"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
